{"cells":[{"cell_type":"markdown","metadata":{"id":"eE-GIZ34Rutp"},"source":["# Input Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60438,"status":"ok","timestamp":1683358775848,"user":{"displayName":"Андрей","userId":"05082256971108686009"},"user_tz":-180},"id":"eLAiFR5nRxnI","outputId":"d318a615-99df-4002-cb86-1fd208c5f6ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow_addons\n","  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typeguard<3.0.0,>=2.7\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n","Installing collected packages: typeguard, tensorflow_addons\n","Successfully installed tensorflow_addons-0.20.0 typeguard-2.13.3\n","Mounted at /content/gdrive\n"]}],"source":["!pip install tensorflow_addons\n","from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","!unzip gdrive/My\\ Drive/archive.zip > /dev/null\n","!unzip gdrive/My\\ Drive/saved_models.zip > /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xGaJozKMSLFz","outputId":"9db90eee-692d-4447-87f4-fcbe53b03246"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov7'...\n","remote: Enumerating objects: 1148, done.\u001b[K\n","remote: Counting objects: 100% (9/9), done.\u001b[K\n","remote: Compressing objects: 100% (9/9), done.\u001b[K\n","remote: Total 1148 (delta 4), reused 0 (delta 0), pack-reused 1139\u001b[K\n","Receiving objects: 100% (1148/1148), 70.42 MiB | 27.50 MiB/s, done.\n","Resolving deltas: 100% (490/490), done.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyyaml==5.1\n","  Downloading PyYAML-5.1.tar.gz (274 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: pyyaml\n","  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyyaml: filename=PyYAML-5.1-cp310-cp310-linux_x86_64.whl size=44090 sha256=739969e034ccfa1fee870f7a8eddd2bfaaa701c72e7ac474ac541d2592990e0d\n","  Stored in directory: /root/.cache/pip/wheels/70/83/31/975b737609aba39a4099d471d5684141c1fdc3404f97e7f68a\n","Successfully built pyyaml\n","Installing collected packages: pyyaml\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 6.0\n","    Uninstalling PyYAML-6.0:\n","      Successfully uninstalled PyYAML-6.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","flax 0.6.9 requires PyYAML>=5.4.1, but you have pyyaml 5.1 which is incompatible.\n","dask 2022.12.1 requires pyyaml>=5.3.1, but you have pyyaml 5.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pyyaml-5.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/facebookresearch/detectron2.git\n","  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-6zrwqa7o\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-6zrwqa7o\n","  Resolved https://github.com/facebookresearch/detectron2.git to commit d4a5f28e01b2babbaba9f90198fb95f5c661cccd\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (8.4.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.6)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.3.0)\n","Collecting yacs>=0.1.8\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.8.10)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n","Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.65.0)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.12.2)\n","Collecting fvcore<0.1.6,>=0.1.5\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting iopath<0.1.10,>=0.1.7\n","  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n","Collecting omegaconf>=2.1\n","  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hydra-core>=1.1\n","  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting black\n","  Downloading black-23.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (23.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.22.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (5.1)\n","Collecting antlr4-python3-runtime==4.9.*\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting portalocker\n","  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.0.9)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.0.7)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.39.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.4)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.3)\n","Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (3.3.0)\n","Collecting mypy-extensions>=0.4.3\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Collecting pathspec>=0.9.0\n","  Downloading pathspec-0.11.1-py3-none-any.whl (29 kB)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.0.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (67.7.2)\n","Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.17.3)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.54.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.8.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.40.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.27.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.4.3)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.3.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2==0.6) (1.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.26.15)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\n","Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n"]}],"source":["! git clone -b mask https://github.com/WongKinYiu/yolov7.git\n","! pip install pyyaml==5.1\n","! pip install 'git+https://github.com/facebookresearch/detectron2.git'\n","! pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15020,"status":"ok","timestamp":1683362778252,"user":{"displayName":"Андрей","userId":"05082256971108686009"},"user_tz":-180},"id":"6T4x5vU8Rutq","outputId":"bdb47da9-84b2-4a80-905e-b728f96f90b6"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Tensorflow version 2.12.0\n","GPU is ON\n"]}],"source":["import tensorflow as tf\n","import tensorflow_addons as tfa\n","from keras import backend\n","from keras import datasets, layers, models, initializers, activations\n","from keras.layers import Conv2D, BatchNormalization, Activation, Conv2DTranspose, Concatenate\n","from functools import reduce\n","import torch\n","from torchvision import transforms\n","from PIL import Image\n","from matplotlib import cm\n","\n","from skimage import measure\n","from skimage import transform\n","from skimage.io import imread, imsave, imshow\n","from skimage.transform import resize\n","from skimage.filters import gaussian\n","from skimage.morphology import dilation, disk\n","from skimage.draw import polygon, polygon_perimeter\n","\n","print(f'Tensorflow version {tf.__version__}')\n","print(f'GPU is {\"ON\" if tf.compat.v1.config.experimental.list_physical_devices(\"GPU\") else \"OFF\" }')\n","\n","\n","import numpy as np\n","%matplotlib inline\n","import matplotlib, random\n","import matplotlib.pyplot as plt\n","import pandas as pd \n","import cv2\n","import os\n","\n","from imutils import paths\n","import pathlib\n","import time\n","import glob\n","from natsort import natsorted\n","import tarfile\n","import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJwsUCQdA3LE"},"outputs":[],"source":["add_signs_classes = {'0':183,\n","                     '1':184,\n","                     '2':185,\n","                     '3':186,\n","                     '4':187,\n","                     '5':188,\n","                     '6':189,\n","                     '7':190,\n","                     '8':191,\n","                     '9':192,\n","                     '10':193,\n","                     '11':194,\n","                     '12':195,\n","                     '13':196,\n","                     '14':197,\n","                     '15':198,\n","                     '16':199,\n","                     '17':200,\n","                     '18':201,\n","                     '19':202,\n","                     '20':203,\n","                     '21':204}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hc2uOfjgA77D"},"outputs":[],"source":["forb_signs_classes = {'0':0,\n","'1':1,\n","'2':10,\n","'3':15,\n","'4':16,\n","'5':17,\n","'6':2,\n","'7':3,\n","'8':32,\n","'9':4,\n","'10':41,\n","'11':42,\n","'12':5,\n","'13':6,\n","'14':7,\n","'15':72,\n","'16':73,\n","'17':74,\n","'18':75,\n","'19':76,\n","'20':77,\n","'21':78,\n","'22':79,\n","'23':8,\n","'24':80,\n","'25':81,\n","'26':82,\n","'27':83,\n","'28':84,\n","'29':85,\n","'30':86,\n","'31':87,\n","'32':88,\n","'33':89,\n","'34':9,\n","'35':90,\n","'36':91,\n","'37':92,\n","'38':93,\n","'39':94,\n","'40':95,\n","'41':96,\n","'42':97,}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0r804M28A8Fx"},"outputs":[],"source":["Warning_signs_classes = {'0':18,\n","'1':19,\n","'2':20,\n","'3':21,\n","'4':22,\n","'5':23,\n","'6':24,\n","'7':25,\n","'8':26,\n","'9':27,\n","'10':28,\n","'11':29,\n","'12':30,\n","'13':31,\n","'14':48,\n","'15':49,\n","'16':50,\n","'17':51,\n","'18':52,\n","'19':53,\n","'20':54,\n","'21':55,\n","'22':56,\n","'23':57,\n","'24':58,\n","'25':59,\n","'26':60,\n","'27':61,\n","'28':62,\n","'29':63,\n","'30':64,\n","'31':65,\n","'32':66,\n","'33':67,\n","'34':68,\n","'35':69,\n","'36':70,\n","'37':71,}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zjvtg6AIA8J5"},"outputs":[],"source":["inf_signs_classes = {'0':148,\n","'1':149,\n","'2':150,\n","'3':151,\n","'4':152,\n","'5':153,\n","'6':154,\n","'7':155,\n","'8':156,\n","'9':157,\n","'10':158,\n","'11':159,\n","'12':160,\n","'13':161,\n","'14':162,\n","'15':163,\n","'16':164,}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mj84NQ57A8Mi"},"outputs":[],"source":["presc_signs_classes = {'0':100,\n","'1':101,\n","'2':102,\n","'3':103,\n","'4':104,\n","'5':105,\n","'6':106,\n","'7':107,\n","'8':108,\n","'9':109,\n","'10':33,\n","'11':34,\n","'12':35,\n","'13':36,\n","'14':37,\n","'15':38,\n","'16':39,\n","'17':40,\n","'18':98,\n","'19':99,}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dIdX1j0hA8O7"},"outputs":[],"source":["priority_signs_classes = {'0':12,\n","'1':13,\n","'2':14,\n","'3':43,\n","'4':44,\n","'5':45,\n","'6':46,\n","'7':47,}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x5eTUN1QA8U7"},"outputs":[],"source":["service_signs_classes = {'0':165,\n","'1':166,\n","'2':167,\n","'3':168,\n","'4':169,\n","'5':170,\n","'6':171,\n","'7':172,\n","'8':173,\n","'9':174,\n","'10':175,\n","'11':176,\n","'12':177,\n","'13':178,\n","'14':179,\n","'15':180,\n","'16':181,\n","'17':182,}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81jCfR3WBE8P"},"outputs":[],"source":["special_instructions_signs_classes = {'0':110,\n","'1':111,\n","'2':112,\n","'3':113,\n","'4':114,\n","'5':115,\n","'6':116,\n","'7':117,\n","'8':118,\n","'9':119,\n","'10':120,\n","'11':121,\n","'12':122,\n","'13':123,\n","'14':124,\n","'15':125,\n","'16':126,\n","'17':127,\n","'18':128,\n","'19':129,\n","'20':130,\n","'21':131,\n","'22':132,\n","'23':133,\n","'24':134,\n","'25':135,\n","'26':136,\n","'27':137,\n","'28':138,\n","'29':139,\n","'30':140,\n","'31':141,\n","'32':142,\n","'33':143,\n","'34':144,\n","'35':145,\n","'36':146,\n","'37':147,}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fyvvtTGKRuts"},"outputs":[],"source":["def run_inference(url, model):\n","    image = cv2.imread(url)\n","    image = cv2.resize(image,(640,384))\n","    image = letterbox(image, 640, stride=64, auto=True)[0] \n","    image = transforms.ToTensor()(image)\n","    image = image.half().to(device)\n","    image = image.unsqueeze(0)\n","    output = model(image)\n","    return output, image\n","  \n","def re_parammetatizing(inf_out, attn, bases, sem_output, image, model):\n","  bases = torch.cat([bases, sem_output], dim=1)\n","  nb,_, height, width = image.shape\n","  names = model.names\n","  pooler_scale = model.pooler_scale\n","\n","  pooler = ROIPooler(output_size=hyp['mask_resolution'], \n","                    scales=(pooler_scale,), \n","                    sampling_ratio=1, \n","                    pooler_type='ROIAlignV2', \n","                    canonical_level=2)\n","                    \n","  # output, output_mask, output_mask_score, output_ac, output_ab\n","  output, output_mask, _, _, _ = non_max_suppression_mask_conf(inf_out, attn, bases, pooler, hyp,\n","                                                            conf_thres=0.25, iou_thres=0.65, merge=False, mask_iou=None) \n","  return  output, output_mask, height, width, names\n","\n","def load_model(weights_path):\n","    model = torch.load(weights_path, map_location=device)['model']\n","    model.eval()\n","\n","    if torch.cuda.is_available():\n","        model.half().to(device)\n","    return model\n","\n","def crop_image(x1,y1,x2,y2, first_image):\n","  im = Image.fromarray(first_image)\n","  img2 = im.crop((x1,y1,x2,y2))\n","  img = np.array(img2)\n","  return img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"odqNU-c6Ruts"},"outputs":[],"source":["def get_colors(mask):\n","    my_array = mask\n","    my_array[my_array == 10 ] = 0\n","    my_array[my_array == 18 ] = 0\n","    my_array[my_array == 12 ] = 0\n","    return my_array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4UO_TvuRuts"},"outputs":[],"source":["img_size = 640\n","semantic_image_size = (img_size, 384)\n","output_size = (img_size, 384)\n","\n","num_classes = 23\n","colors = np.array([(0, 0, 0),\n","                   (70, 70, 70),\n","                   (100, 40, 40),\n","                   (55, 90, 80),\n","                   (220, 20, 60),\n","                   (153, 153, 153),\n","                   (157, 234, 50),\n","                   (128, 64, 128),\n","                   (244, 35, 232),\n","                   (107, 142, 35),\n","                   (0, 0, 142),\n","                   (102, 102, 156),\n","                   (220, 220, 0),\n","                   (70, 130, 180),\n","                   (81, 0, 81),\n","                   (150, 100, 100),\n","                   (230, 150, 140),\n","                   (180, 165, 180),\n","                   (250, 170, 30), \n","                   (110, 190, 160),\n","                   (170, 120, 50),\n","                   (45, 60, 150),\n","                   (145, 170, 100)])"]},{"cell_type":"markdown","metadata":{"id":"t81foTZ1Rutt"},"source":["# Load Panoptic Model"]},{"cell_type":"markdown","metadata":{"id":"xNu0f3o6Rutt"},"source":["## Load Semantic head (EfficientUDet+)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1683362778254,"user":{"displayName":"Андрей","userId":"05082256971108686009"},"user_tz":-180},"id":"_Vv-Dp4yRutt","outputId":"84dcf217-ee71-4d63-e034-44bc8c028882"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tensorflow version 2.12.0\n","GPU is ON\n"]}],"source":["from EfficientUDet import Semantic_model\n","from keras.optimizers import SGD, Adam\n","from keras import metrics\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YmfRvgTSOVz"},"outputs":[],"source":["opt = Adam(learning_rate=0.0003)\n","Semantic_model = Semantic_model(0, num_classes=num_classes)\n","Semantic_model.compile(optimizer=opt,loss='binary_crossentropy', metrics=[metrics.OneHotIoU(num_classes,[i for i in range(0,num_classes)])])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xadFfai7dDyv"},"outputs":[],"source":["Semantic_model.load_weights('./gdrive/MyDrive/sem_weights(0.6044).h5')\n"]},{"cell_type":"markdown","metadata":{"id":"7Hqyzkt3Rutt"},"source":["## Load Instance Head (YoloV7)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2223,"status":"ok","timestamp":1683362787504,"user":{"displayName":"Андрей","userId":"05082256971108686009"},"user_tz":-180},"id":"-yAiXMEiSZxC","outputId":"f902e5a4-0687-43e8-c31e-cafc5ec89794"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/yolov7\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100 87.0M  100 87.0M    0     0  46.0M      0  0:00:01  0:00:01 --:--:-- 69.9M\n"]}],"source":["%cd yolov7\n","! curl -L https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-mask.pt -o yolov7-mask.pt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Bt9a8RgRutu"},"outputs":[],"source":["from utils.datasets import letterbox\n","from utils.general import non_max_suppression_mask_conf\n"," \n","from detectron2.modeling.poolers import ROIPooler\n","from detectron2.structures import Boxes\n","from detectron2.utils.memory import retry_if_cuda_oom\n","from detectron2.layers import paste_masks_in_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQRWM8CINpMl"},"outputs":[],"source":["import yaml\n","with open('../yolov7/data/hyp.scratch.mask.yaml') as f:\n","    hyp = yaml.load(f, Loader=yaml.FullLoader)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","Instance_model = load_model('yolov7-mask.pt')"]},{"cell_type":"markdown","metadata":{"id":"y7k09GFSRutu"},"source":["## Load Classification Heads (ResNets)"]},{"cell_type":"markdown","metadata":{"id":"X-tjCYU1Rutu"},"source":["### Traffic Signs Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zpqePgQBRutu"},"outputs":[],"source":["from keras import models\n","import tensorflow_addons as tfa\n","\n","# categorical model\n","Trafic_Sign_model = models.load_model('../saved_models/Trafic_Signs_CNN_categorical.h5')\n","\n","# Signs model\n","add_signs_model = models.load_model('../saved_models/Trafic_Signs_CNN_add_signs.h5')\n","forb_signs_model = models.load_model('../saved_models/Trafic_Signs_CNN_forb_signs.h5')\n","inf_signs_model = models.load_model('../saved_models/Trafic_Signs_CNN_inf_signs.h5')\n","presc_signs_model = models.load_model('../saved_models/Trafic_Signs_CNN_presc_signs.h5')\n","priority_signs_model = models.load_model('../saved_models/Trafic_Signs_CNN_priority_signs.h5')\n","service_signs_model = models.load_model('../saved_models/Trafic_Signs_CNN_service_signs.h5')\n","special_instructions_signs_model = models.load_model('../saved_models/Trafic_Signs_CNN_special_instructions_signs.h5')\n","Warning_signs_model = models.load_model('../saved_models/Trafic_Signs_CNN_Warning_signs.h5')"]},{"cell_type":"markdown","metadata":{"id":"et0CyVZMRutu"},"source":["### Traffic Lights Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jw0stNl4Rutu"},"outputs":[],"source":["Trafic_Lights_model = models.load_model('../saved_models/Trafic_Lights_CNN.h5')\n"]},{"cell_type":"markdown","metadata":{"id":"_8jzoQnIRutv"},"source":["### Road Type Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ck1N9b1kRutv"},"outputs":[],"source":["\n","\n","# categorical model\n","Road_Type_model = models.load_model('../saved_models/Road_Type_AllClasses_CNN.h5')\n","\n","# road model\n","Road_Type_Asphalt_model = models.load_model('../saved_models/Road_Type_Asphalt_CNN.h5')\n","Road_Type_Concrete_model = models.load_model('../saved_models/Road_Type_Concrete_CNN.h5')\n","Road_Type_Gravel_model = models.load_model('../saved_models/Road_Type_Gravel_CNN.h5')\n","Road_Type_Mud_model = models.load_model('../saved_models/Road_Type_Mud_CNN.h5')\n","Road_Type_Snow_model = models.load_model('../saved_models/Road_Type_Snow_CNN.h5')"]},{"cell_type":"markdown","metadata":{"id":"Hmzh8Qt9Rutv"},"source":["### Road Line Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OOFxaMk_Rutv"},"outputs":[],"source":["Road_Line_model = models.load_model('../saved_models/Road_Line_CNN.h5')"]},{"cell_type":"markdown","metadata":{"id":"-cuo6k3NRutv"},"source":["# Model Predicition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85fEj18GObHS"},"outputs":[],"source":["def instance_model_prediction(image_path, model):\n","    \n","    output, image = run_inference(image_path, model)\n","    inf_out = output['test']\n","    attn = output['attn']\n","    bases = output['bases']\n","    sem_output = output['sem']\n","\n","    output, output_mask, height, width, names = re_parammetatizing(inf_out, attn, bases, sem_output, image, model)\n","    pred, pred_masks = output[0], output_mask[0]\n","    base = bases[0]\n","    bboxes = Boxes(pred[:, :4])\n","\n","    original_pred_masks = pred_masks.view(-1, hyp['mask_resolution'], hyp['mask_resolution'])\n","    original_image = np.moveaxis(image.cpu().numpy().squeeze(), 0, 2).astype('float32')\n","    original_image = cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR)\n","\n","    pred_masks = retry_if_cuda_oom(paste_masks_in_image)(original_pred_masks, bboxes, (height, width), threshold=0.5)\n","\n","    pred_masks_np = pred_masks.detach().cpu().numpy()\n","    pred_cls = pred[:, 5].detach().cpu().numpy()\n","    pred_conf = pred[:, 4].detach().cpu().numpy()\n","    nimg = image[0].permute(1, 2, 0) * 255\n","    nimg = nimg.cpu().numpy().astype(np.uint8)\n","    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n","    nbboxes = bboxes.tensor.detach().cpu().numpy().astype(np.int)\n","    return pred_masks_np, pred_cls, pred_conf, nimg, nbboxes, height, width, original_image, names\n","\n","def semantic_model_prediction(img, output_size):\n","    image = cv2.imread(img)\n","    img = cv2.resize(image, output_size)\n","    pred = Semantic_model(np.array([img]))\n","    pred = np.argmax (pred, axis=-1) [0, :,:]\n","    return pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ipKvQ4LERutv"},"outputs":[],"source":["def panoptic_fusion(pred, original_image, pred_masks_np, height, width, pred_conf):\n","\n","    panoptic_pred = get_colors(pred)\n","    for i in range(len(instance_pred_masks_np)):\n","      if pred_conf[i]>=0.5:\n","        if instance_pred_cls[i] == 2 or instance_pred_cls[i] == 7:\n","          panoptic_pred[instance_pred_masks_np[i]] = panoptic_pred[instance_pred_masks_np[i]]  + np.array(10, dtype=np.uint8)\n","        if instance_pred_cls[i] == 9:\n","          panoptic_pred[instance_pred_masks_np[i]] = panoptic_pred[instance_pred_masks_np[i]]  + np.array(12, dtype=np.uint8)\n","    \n","    return panoptic_pred\n","\n","def panoptic_bounding(panoptic_image, original_image, pred_masks_np, names, pred_cls, pred_conf, image_path, nbboxes):\n","    height,width = panoptic_image.shape[:2]\n","    img = cv2.imread(image_path)\n","    original_image = cv2.resize(img, (width,height))\n","    orig_img = original_image.copy()\n","    pred_img = cv2.addWeighted(colors[panoptic_image].astype(np.int32), 0.8,original_image.astype(np.int32), 1.,0)\n","    for i in range(len(pred_masks_np)):\n","        pred_img = pred_img.copy()\n","        if pred_conf[i]>=0.5:\n","          color = [np.random.randint(255), np.random.randint(255), np.random.randint(255)]\n","          pred_img = cv2.rectangle(pred_img, (nbboxes[i][0], nbboxes[i][1]), (nbboxes[i][2], nbboxes[i][3]), color, 2)\n","          if int(pred_cls[i]) == 9:\n","            traffic_light_image = crop_image(nbboxes[i][0], nbboxes[i][1], nbboxes[i][2], nbboxes[i][3], original_image)\n","            %matplotlib inline\n","            traffic_light_image = cv2.cvtColor(traffic_light_image, cv2.COLOR_BGR2RGB)\n","            # plt.imshow(traffic_light_image)\n","            # plt.savefig('../%s.png' % (random.randint(0,100)),bbox_inches='tight')\n","            pred_label = traffic_sign_prediction(traffic_light_image)\n","            label = '%s %.3f' % (pred_label, pred_conf[i])\n","          else: \n","            label = '%s %.3f' % (names[int(pred_cls[i])], pred_conf[i])\n","          t_size = cv2.getTextSize(label, 0, fontScale=0.1, thickness=1)[0]\n","          c2 = nbboxes[i][0] + t_size[0], nbboxes[i][1] - t_size[1] - 3\n","          pred_img = cv2.rectangle(pred_img, (nbboxes[i][0], nbboxes[i][1]), c2, color, -1, cv2.LINE_AA)\n","          pred_img = cv2.putText(pred_img, label, (nbboxes[i][0], nbboxes[i][1] - 2), 0, 0.5, [255, 255, 255], thickness=1, lineType=cv2.LINE_AA)\n","    label_line = str(road_line_prediction(orig_img))\n","    color = [np.random.randint(255), np.random.randint(255), np.random.randint(255)]\n","    pred_img = cv2.rectangle(pred_img, (0, 250), (250, 320), color, 2, cv2.LINE_AA)\n","    pred_img = cv2.putText(pred_img, label_line, (0, 250 - 2), 0, 0.5, [255, 255, 255], thickness=1, lineType=cv2.LINE_AA)\n","\n","    label_road = str(road_type_prediction(orig_img))\n","    color = [np.random.randint(255), np.random.randint(255), np.random.randint(255)]\n","    pred_img = cv2.rectangle(pred_img, (0, 250), (550, 320), color, 2, cv2.LINE_AA)\n","    pred_img = cv2.putText(pred_img, label_road, (400, 250 - 2), 0, 0.5, [255, 255, 255], thickness=1, lineType=cv2.LINE_AA)\n","    panoptic_bboxes_image = np.array(pred_img)\n","    \n","    return panoptic_bboxes_image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6RjRBD445Rdd"},"outputs":[],"source":["def traffic_sign_prediction(image):\n","  traffic_light_image = cv2.resize (image, (30,60))\n","  traffic_light_image = cv2.cvtColor(traffic_light_image, cv2.COLOR_BGR2RGB)\n","  traffic_light_image = np.array(traffic_light_image)\n","  img_batch = np.expand_dims(traffic_light_image,0)\n","  prediction = Trafic_Lights_model.predict(img_batch)\n","  prediction = np.argmax(prediction, axis=1)\n","  if prediction[0] == 3:p = 'yellow_traffic_light_signal'\n","  if prediction[0] == 1:p = 'red_traffic_light_signal'\n","  if prediction[0] == 2:p = 'green_traffic_light_signal' \n","  if prediction[0] == 0:p = 'back_traffic_light'\n","  return p\n","\n","def road_line_prediction(image):\n","  road_image = image[300:450, 0:250]\n","  road_image = cv2.cvtColor(road_image,cv2.COLOR_BGR2RGB) ## переводим в оттенки серого\n","  road_image = cv2.resize(road_image, (256,256))\n","\n","  plt.imshow(road_image)\n","  road_image = np.array(road_image)\n","  img_batch = np.expand_dims(road_image,0)\n","  prediction = np.argmax(Road_Line_model.predict(img_batch), axis=1)\n","  if prediction[0] == 0:p = 'Botts-dots '\n","  if prediction[0] == 3:p = 'continuous'\n","  if prediction[0] == 2:p = 'continuous_yellow'\n","  if prediction[0] == 1:p = 'dashed'\n","  if prediction[0] == 4:p = 'double_continuous'\n","  if prediction[0] == 5:p = 'double_dashed'\n","\n","  return p\n","\n","def traffic_signs_prediction(image):\n","  image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB) ## переводим в оттенки серого\n","  image_resized = transform.resize(image, (60,60))\n","  img_batch = np.expand_dims(image_resized,0)\n","\n","  # Начинаем предсказание категории знака\n","  categorical_prediction = Trafic_Sign_model.predict(img_batch)\n","  categorical_prediction = np.argmax(categorical_prediction, axis=1)\n","  # Начинаем предсказание знака внутри категории\n","  if categorical_prediction[0] == 2:\n","      image_resized = transform.resize(image, (60,90))\n","      img_batch = np.expand_dims(image_resized,0)\n","      \n","      add_signs_prediction = add_signs_model.predict(img_batch)\n","      add_signs_prediction = np.argmax(add_signs_prediction, axis=1)\n","      \n","      return add_signs_classes[str(add_signs_prediction[0])]\n","      \n","  if categorical_prediction[0] == 0:\n","      forb_signs_prediction = forb_signs_model.predict(img_batch)\n","      forb_signs_prediction = np.argmax(forb_signs_prediction, axis=1)\n","      \n","      return  forb_signs_classes[(str(forb_signs_prediction[0]))]\n","\n","  if categorical_prediction[0] == 3:\n","      inf_signs_prediction = inf_signs_model.predict(img_batch)\n","      inf_signs_prediction = np.argmax(inf_signs_prediction, axis=1)\n","      \n","      return  inf_signs_classes[(str(inf_signs_prediction[0]))]\n","            \n","  if categorical_prediction[0] == 4:\n","      presc_signs_prediction = presc_signs_model.predict(img_batch)\n","      presc_signs_prediction = np.argmax(presc_signs_prediction, axis=1)\n","      return  presc_signs_classes[(str(presc_signs_prediction[0]))]\n","      \n","  if categorical_prediction[0] == 5:\n","      image_resized = transform.resize(image, (70,70))\n","      img_batch = np.expand_dims(image_resized,0)\n","      \n","      priority_signs_prediction = priority_signs_model.predict(img_batch)\n","      priority_signs_prediction = np.argmax(priority_signs_prediction, axis=1)\n","      \n","      return  priority_signs_classes[(str(priority_signs_prediction[0]))]\n","\n","  if categorical_prediction[0] == 6:\n","      image_resized = transform.resize(image, (120,80))\n","      img_batch = np.expand_dims(image_resized,0)\n","      \n","      service_sign_prediction = service_signs_model.predict(img_batch)\n","      service_sign_prediction = np.argmax(service_sign_prediction, axis=1)\n","      \n","      return service_signs_classes[(str(service_sign_prediction[0]))]\n","         \n","  if categorical_prediction[0] == 7:\n","\n","      special_instructions_signs_prediction = special_instructions_signs_model.predict(img_batch)\n","      special_instructions_signs_prediction = np.argmax(special_instructions_signs_prediction, axis=1)\n","      \n","      return  special_instructions_signs_classes[(str(special_instructions_signs_prediction[0]))]\n","\n","          \n","  if categorical_prediction[0] == 1:\n","      Warning_signs_prediction = Warning_signs_model.predict(img_batch)\n","      Warning_signs_prediction = np.argmax(Warning_signs_prediction, axis=1)\n","      \n","      return Warning_signs_classes[(str(Warning_signs_prediction[0]))]\n","\n","def road_type_prediction(image):\n","  road_image = cv2.resize(image[300:450, 0:550], (240,360))\n","  road_image = cv2.cvtColor(road_image,cv2.COLOR_BGR2RGB) ## переводим в оттенки серого\n","  plt.imshow(road_image)\n","  img_batch = np.expand_dims(road_image,0)\n","  road_class = np.argmax(Road_Type_model.predict(img_batch), axis=1)\n"," \n","  # Начинаем предсказание знака внутри категории\n","  if road_class[0] == 0:\n","      Road_Type_Snow_prediction = Road_Type_Snow_model.predict(img_batch)\n","      Road_Type_Snow_prediction = np.argmax(Road_Type_Snow_prediction, axis=1)\n","      \n","      if Road_Type_Snow_prediction[0] ==0: p = 'fresh_snow'\n","      if Road_Type_Snow_prediction[0] ==1: p = 'ice'\n","      if Road_Type_Snow_prediction[0] ==2: p = 'melted_snow'\n","\n","      return p\n","  if road_class[0] == 1:\n","      Road_Type_Mud_prediction = Road_Type_Mud_model.predict(img_batch)\n","      Road_Type_Mud_prediction = np.argmax(Road_Type_Mud_prediction, axis=1)\n","\n","      if Road_Type_Mud_prediction[0] ==0: p = 'dry_mud'\n","      if Road_Type_Mud_prediction[0] ==1: p = 'water_mud'\n","      if Road_Type_Mud_prediction[0] ==2: p = 'wet_mud'\n","      \n","      return p\n","  \n","  if road_class[0] == 2:\n","    Road_Type_Gravel_prediction = Road_Type_Gravel_model.predict(img_batch)\n","    Road_Type_Gravel_prediction = np.argmax(Road_Type_Gravel_prediction, axis=1)\n","\n","    if Road_Type_Gravel_prediction[0] ==0: p = 'dry_gravel'\n","    if Road_Type_Gravel_prediction[0] ==1: p = 'water_gravel'\n","    if Road_Type_Gravel_prediction[0] ==2: p = 'wet_gravel'\n","    \n","    return p\n","  if road_class[0] == 3:\n","    Road_Type_Concrete_prediction = Road_Type_Concrete_model.predict(img_batch)\n","    Road_Type_Concrete_prediction = np.argmax(Road_Type_Concrete_prediction, axis=1)\n","    \n","    if Road_Type_Concrete_prediction[0] ==0: p = 'dry_concrete'\n","    if Road_Type_Concrete_prediction[0] ==1: p = 'water_concrete'\n","    if Road_Type_Concrete_prediction[0] ==2: p = 'wet_concrete'\n","    \n","    return p\n","      \n","  if road_class[0] == 4:\n","      Road_Type_Asphalt_prediction = Road_Type_Asphalt_model.predict(img_batch)\n","      Road_Type_Asphalt_prediction = np.argmax(Road_Type_Asphalt_prediction, axis=1)\n","      if Road_Type_Asphalt_prediction[0] ==0: p = 'dry_asphalt'\n","      if Road_Type_Asphalt_prediction[0] ==1: p = 'water_asphalt'\n","      if Road_Type_Asphalt_prediction[0] ==2: p = 'wet_asphalt'\n","      return  p"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4272,"status":"ok","timestamp":1683363080840,"user":{"displayName":"Андрей","userId":"05082256971108686009"},"user_tz":-180},"id":"8ItpaV2mXLpf","outputId":"65a88027-31f1-4890-ac0b-a142818b0511"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (8.3.1)\n"]}],"source":["!pip install natsort"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GfLLt2MXOQk"},"outputs":[],"source":["from natsort import natsorted\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1isN4cPGPQKt_Q41B6BYcMh7y6fIg4W9D"},"id":"x7ZUj3LbiuTV","outputId":"3b99c0d5-5bb1-4f27-f891-a1cd754bd049","executionInfo":{"status":"ok","timestamp":1683363132204,"user_tz":-180,"elapsed":51367,"user":{"displayName":"Андрей","userId":"05082256971108686009"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["images_path = natsorted(glob.glob('../dataA/dataA/CameraRGB/*.png'))\n","masks_path = natsorted(glob.glob('../dataA/dataA/CameraSeg/*.png'))\n","count = 0\n","all_pred = []\n","for i in range(138,151):\n","  try:\n","    image_path = str(images_path[i]) \n","    semantic_pred = semantic_model_prediction (image_path, semantic_image_size)\n","    semantic_pred_2 = semantic_model_prediction (image_path, semantic_image_size)\n","\n","    true_mask = cv2.imread(masks_path[i])\n","    true_mask = cv2.resize(true_mask, semantic_image_size)\n","    true_mask = true_mask[:,:,2]\n","    \n","    instance_pred_masks_np, instance_pred_cls, instance_pred_conf, nimg, nbboxes, height, width,original_image, names = instance_model_prediction(image_path, Instance_model)\n","    panoptic_image = panoptic_fusion(semantic_pred, original_image, instance_pred_masks_np, height, width,instance_pred_conf)\n","\n","    iou = tf.keras.metrics.IoU(num_classes=23, target_class_ids=[0,1,7,8,9,10,11,12,15,16,18,19,20,21,22])\n","    iou.update_state(panoptic_image[:310], true_mask[:310])\n","    print(iou.result().numpy())\n","    all_pred.append(iou.result().numpy())\n","\n","    %matplotlib inline\n","\n","    plt.figure(figsize = (20,20))\n","    plt.subplot (1,2,1)\n","    plt.title(\"Original Image\")\n","    plt.axis('off')\n","    plt.imshow (original_image)\n","\n","    # plt.subplot (1,5,2)\n","    # plt.title(\"Ground Truth\")\n","    # plt.axis('off')\n","    # plt.imshow (colors [true_mask])\n","\n","    # plt.subplot (1,5,3)\n","    # plt.title(\"Semantic Prediction\")\n","    # plt.axis('off')\n","    # plt.imshow (colors [semantic_pred_2])\n","\n","    # plt.subplot (1,5,4)\n","    # plt.title(\"Panoptic Prediction\")\n","    # plt.axis('off')\n","    # plt.imshow (colors[panoptic_image])\n","\n","    plt.subplot (1,2,2)\n","    plt.title(\"Hydra Panoptic Prediction\")\n","    plt.axis('off')\n","    plt.imshow (panoptic_bounding(panoptic_image, original_image, instance_pred_masks_np, names, instance_pred_cls, instance_pred_conf, image_path, nbboxes))\n","\n","    plt.savefig('../gdrive/MyDrive/z/panoptic %s.png' % i,bbox_inches='tight')\n","    plt.show()\n","  except:\n","    pass\n","print(sum(all_pred)/len(all_pred))\n"]},{"cell_type":"code","source":[],"metadata":{"id":"1ksriPEf_Rz3"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}